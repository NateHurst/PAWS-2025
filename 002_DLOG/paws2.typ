// Example usage of the homework template
#import "template.typ": *

// Page setup (MUST be at document level, not in function)
#set page(
  margin: (left: 1in, right: 1in),
  numbering: "1",
  footer: [
    #align(center)[#context [#counter(page).display()]]
  ],
)
#set par(first-line-indent: 0pt)
#set block(spacing: 2.5em)

// Create the homework title
#homework_title("Problem Set 2", "PAWS 2025", "Nathaniel Hurst")


// Numbered problems with yellow boxes and circled numbers
#numbered_problem(1)[
    The order of $2 in FF_(71)^*$ is $35$. Charlie uses the subgroup generated by $g = 2$, his public key is $g_c = 29$. Use the baby-step giant-step algorithm to compute an integer $c$ such that $g_c equiv g^c space (mod 71)$.
 
]

First we compute $m = ⌊sqrt(35)⌋+1=6$. Then we compute the sequence #align(center,$g_0 = 1 space space space space g_1 = 2 space space space space g_2 = 4 space space space space g_3 = 8 space space space space g_4 = 16 space space space space g_5 = 32$) for our baby steps and the the sequence #align(center,$A_0 = 29 space space space space A_1 = 29 dot g^(-6) = 6 space space space space A_2 = 29 dot (g^(-6))^2 = 60 space space space space A_3 = 29 dot (g^(-6))^3 = 32$) For our giant steps (note we stopped at the third giant step as we found a collision $g_5 = A_3$). Thus our discrete log is given by $g^(6 dot 3 + 5)= 29$, or $"dlog"_g (29)=6 dot 3 +5 = 23$.

#numbered_problem(2)[

We have seen in Remark 2.12 (of the lecture notes) that one may reduce the size of the required memory at the cost of increasing the overall runtime of the algorithm. In this exercise, the goal is to achieve the opposite: decreasing the runtime at the cost of increasing the required memory.

(a) We have shown that Algorithm 1 (of the lecture notes) requires $O(sqrt(q))$ multiplications. More concretely, show that the average runtime is given by $T = 3 / 2 sqrt(q)$ (if the exponent $a$ is chosen uniformly at random).
        
Now consider a variant of Algorithm 1, where the baby steps and giant steps are computed in parallel, and all values $g_i, A_i$ for $0 <= i <= n <= m$ are stored until the match is found for some $n$.
        
(b) What is the average runtime of this variant of Algorithm 1? What is the required memory? 
        
*Hint:* You can use (or prove if you are familiar with probability theory) that for two integers $i, j$ uniformly chosen at random from $(0, dots, m)$, the expected value of $max(i,j) approx 2/3m$.
        
]

(a) Observe that we must always do $m = ⌊sqrt(q)⌋+1$ multiplications to generate our baby steps. Then we need to do at maximum another $m = ⌊sqrt(q)⌋+1$ giant steps, at each point checking for a collision in our baby and giant steps. The expected number of giant step computations we will have to do is $(sqrt(q))/2$ before we hit a collision (if $a$ is chosen uniformly at random), and combining that with the guaranteed $sqrt(q)$ computations we have that the average runtime of this algorithm is $3/2sqrt(q)$.
#line(length: 15%)
(b) Using the hint we can calculate the expected time before the first collision. To do so we fix two integers $i,j in {0,dots,m}$ such that $"baby_step"_i = "giant_step"_j$ is the first collision. Then we will hit this collision in our computations after $max{i,j} = 2/3m$ steps (the computations are running in parallel, so $4/3m$ computations total). Thus the average runtime is $2/3m approx 2/3 sqrt(q)$ and the required memory is $4/3m approx 4/3 sqrt(q)$.

#numbered_problem(3)[

Implement the baby-step giant-step algorithm and use it to solve the DLP instances from Exercise 5 of the first exercise set (copied again here). How does the running time compare to the $log$ function in SageMath? Which algorithm is used in SageMath to solve the DLP?

    In all of these, the public parameters are a prime $p = 2q + 1$, and the element $g = 4 in FF_p^*$ with order $q$:
    
(a) $q = 4294967681 approx 2^(32)$, 
        $A = 5104411285$, $B = 7620748646$.
        
(b) $q = 18446744073709552109 approx 2^(64)$, 
        $A = 17485644247020728566$, $B = 17485644247020728566$.
        
(c) $q = 340282366920938463463374607431768219863 approx 2^(128)$, 
        $A = 15855669586157245378211095347605706305$, 
        $B = 643791185530305885858740134946520672205$

]

To save space and computing time we only reproduce the first discrete logarithm with the following baby-step giant-step algorith (code can be found in baby-step_giant-step.ipynb in the Scripts folder of my github repo:

#figure(
  image("images/bsgs.png", width: 100%)
)

The algorithm used by SageMath is defaulted to the baby-step giant-step but it has many algorithms you can switch to in order to tailor to your input.

#numbered_problem(4)[
Use Pollard's rho algorithm to compute $c$ such that $2^c equiv 29 space (mod 71)$, i.e., use Pollard's rho algorithm to do Exercise 1 in this Problem Set. What are $T$ and $L$? How does the value of $T + L$ compare to the expected value given in Theorem 2.16?
]
We will use the function given by Pollard, that is $f(x) = x dot A$ if $0<x < p/3$,  $f(x) = x^2$ if $p/3 < x < (2p)/3$, and $f(x) = x dot g$ if $(2p)/3 < x < p$. Then we start with $x_0=1$, giving us the sequence #align(center,$x_1 = f(x_0) = A = 29 space space space space space space x_2 = f(x_1)=A^2 = 60 space space space space space space x_3 = f(x_2) = A^2 dot g = 60 dot 2 = 49$) #align(center, $x_4 = f(x_3) = A^2 dot g^2 = 49 dot 2 = 27 space space space space space space x_5 = f(x_4) = A^4 dot g^4  = 27^2 = 19 space space space space space space$) #align(center, $x_6 = f(x_5) = A^5 dot g^4 = 19 dot 29 = 54 space space space space space space x_7 = f(x_6)= A^5 dot g^5 = 37 space space space space space space$) #align(center, $x_8 = f(x_7)=A^10 dot g^10 = 37^2 = 20 space space space space space space x_9 = f(x_8)= A^11 dot g^10 = 20 dot 29 = 12$) #align(center, $x_10 = f(x_9) = A^12 dot g^10 = 12 dot 29 = 64 space space space space space space x_11= f(x_10) = A^12 dot g^11 = 64 dot 2 = 57$) #align(center, $x_12 = f(x_11) = A^12 dot g^12 = 57 dot 2 = 43 space space space space space space x_13 = f(x_12) = A^24 dot g^24 = 43^2 = 3$) #align(center, $x_14 = f(x_13) = A^25 dot g^24 =3 dot 29 = 16 space space space space space space x_15 = f(x_14) = A^26 dot g^24 = 16 dot 29 = 38$) #align(center, $x_16 = f(x_15) = A^52 dot g^48 = 38^2 = 24 space space space space space space x_17 = f(x_16) = A^104 dot g^96 = 24^2 = 8$) #align(center, $x_18 = f(x_17) = A^105 dot g^96 = 8 dot 29 = 19$)

Giving us our collision $A^4 dot g^4 = x_5 = x_19 = A^105 dot g^96$. Hence the discrete logarithm is given by $a equiv (4-105)^(-1) dot (96-4) equiv 23 space (mod "ord"(g))$. Which is the same as in problem $1$. In this case we see that $T = 5$ and $L = 19-5 = 14$, giving us $T+L = 19$. By Theorem $2.16$ the expected value of $T+L$ is $E(T+L) approx 1.2533 dot sqrt(71) approx 10.56$. Since this is a Las Vegas Model I am not suprised that we were not close to the expected value, we got unlucky.

#numbered_problem(5)[
Explain why $f(x) = x^2$ is a bad (inefficient) choice of function for Pollard’s rho algorithm (say, with initial value $x_0 = g dot A$, where the order of $g$ is odd).
]

The first thing that makes this function inefficient is that it is not psuedorandom, and so we cannot apply the birthday paradox to get its time complexity. Second if $"ord(g)" = 2^n -1$ for some $n$ then it is possible that the algorithm will have no "tail" as it could continuously loop back to the starting value $x_0$.

#numbered_problem(6)[
*Pollard’s $rho$ for Integer Factorization*. You already know that Pollard’s $rho$ algorithm can be used to solve the *discrete logarithm problem* by exploiting cycle detections. The same idea can be adapted to *factor composite integers*, i.e. find prime numbers $p_1, dots, p_r$ such that $n = p_1 dots p_r$.

Let $n$ be a composite integer. Consider the sequence
#align(center, $x_(k+1) equiv pi(x_k) := x^2 + 1 space (mod n)$)

Because there are only $n$ residues, the sequence eventually repeats. If a prime $p | n$ causes two values to collide modulo $p$ before they collide modulo $n$, then
#align(center, $gcd(|x_i - x_j|, n)$)
   
can reveal a non-trivial factor of $n$.

(a) Show that if $x_i equiv x_j space (mod p)$, then $x_(i+k) equiv x_(j+k) space (mod p)$ for all $k >= 0$.

(b) Implement Pollard’s $rho$ method of factorization in Sagemath and use it to factor the following integers:
        
#align(center, $n_1 &= 1007 space space space space 
            n_2 &= 8051 space space space space
            n_3 &= 10403 space space space space
            n_4 &= 5545419598547562675200 space space space space
            $)#align(center,$n_5 &= 62636019807439769674752  space space space space
            n_6 &= 19783 space space space space
            n_7 &= 16310011$)

        (Yes, they are ordered from easy to hard :), do the next one if you want to understand why)

(c) Use Theorem 2.16 to estimate the expected number of steps needed to factor $n$, assuming $pi$ to be a random permutation. Note: it may be helpful to use the Theorem on a function different from $pi$, since you are looking for a collision modulo one of the prime factors of $n$.

(d) Explain why Pollard’s $rho$ is particularly effective when $n$ has many small prime factors, even if $n$ itself is large.

]

(a) Observe that if $x_i equiv x_j space (mod p)$ then $x_(i+1) equiv (x_i)^2+1 equiv (x_j)^2+1 equiv x_(j+1) space (mod p)$, and so repeating inductively we see that $x_(i+k) equiv x_(j+k) space (mod p)$ for all $k >= 0$.
#line(length: 15%)
(b) We use the following code to compute the factorization (which can be found in the file pollard_integer_factorization.ipynb in my github repo scripts folder):

#figure(
  image("images/pollards_factorization.png", width: 100%),
  caption: [
    Code (we use trial division to factor numbers less than $10$ as the algorithm gets stuck in a meaningless loop in this case)
  ],
)

Which yields: #align(center, $n_1 = 1007 = 19 dot 53$) #align(center, $n_2 = 8051 = 83 dot 97$) #align(center, $n_3 = 10403 = 101 dot 103$) #align(center, $n_4 = 5545419598547562675200 = 2^10 dot 5^2 dot 7 dot 19 dot 1628706414047099 $) #align(center, $n_5 = 62636019807439769674752 = 2^10 dot 3^4 dot 755160346829665433$) #align(center, $n_6 = 19783 = 73 dot 271$) #align(center, $n_7 =16310011 = 67 dot 243433$)

The algorithm runs astonishingly quickly, with factorization time for the hardest example ($n_7$) being only $0.4$ ms.
#line(length: 15%)
(c) Let $n = p_1 dots p_k$ be the prime factorization of $n$, where we allow for repeats in the primes $p_i$. Then the number of steps before we get a collision modulo $p_i$ is (by Theorem 2.16) $approx 1.2533 dot sqrt(p_i)$. Now we must get a collision for each prime power $p_i$, and so combining all the steps gives us that the total number of expected steps to factor $n$ is about $1.2533 (sum_(i=1)^k sqrt(p_i))$, where $p_i$ are the primes dividing $n$ (with repeats if it appears more than once in the prime factorization).
#line(length: 15%)
(d) Pollard's $rho$ for integer factorization is particularly effective when $n$ has many small prime factors as when a prime $p | n$ is small the number of equivalence classes mnodulo $p$ is small, and hence we are more likely to get a collision modulo $p$. Take for example $p=2$, we will get a collision modulo $2$ as soon as both $x_i$ and $x_(2 i)$  are both even or both odd, which will happen often. 

#numbered_problem(7)[
Implement the Pollard rho algorithm in Sagemath and use it solve the DLP instances from Exercise 3. Compare the run times with your baby-step giant-step implementation.
]

This was my implementation, code can be found in the scripts folder in my github repo under pollard_rho.ipynb:

#figure(
  image("images/pollardrho.png", width: 100%)
)

This took longer to run than the baby-step giant-step algorithm, and while they are both exponential in time complexity, I hypothesize (with no evidence) that the baby-step giant-step algorithm works faster when the numbers are relatively small.
#numbered_problem(8)[
Use index calculus to compute an integer $c$ such that $2^c equiv 29 space (mod 71)$ (i.e., use index calculus to solve #1 on this problem set). Use factor base $cal(P)_B = {2, 3, 5}$ and the sequence of integers $e = 5, 13, 32, 19$.

]

First we use the sequence $e  =5,13,32,19$ and compute the prime factorization of $hat(g^e slash A) in ZZ$. We continue this until we have the following $4$ relations (since $\# B = 3$): #align(center, $hat(g^5 slash A) = 6 = 2 dot 3 space space space space space space space hat(g^13 slash A) = 45 = 3^2 dot 5 space space space space space space space hat(g^32 slash A) = 15 = 3 dot 5 space space space space space space space hat(g^19 slash A) = 40 = 2^3 dot 5$) Giving us the linear system in $ZZ slash 70 ZZ$: #align(center, $5 = 1x_2+1x_3+0x_5+a$) #align(center, $13 = 0x_2+2x_3+1x_5+a$) #align(center, $32 = 0x_2+1x_3+1x_5+a$) #align(center, $19 = 3x_2+0x_3+1x_5+a$) Now solving this system using SageMath we get the answer $a equiv 23 space (mod 35)$, which is the same as in problem $\#1$.

#numbered_problem(9)[
When looking at Example 2.19 from the lecture notes, can you also solve for $x_2$, $x_3$ and $x_5$? Explain your observations.

]

It is possible to also solve for $x_2$, $x_3$, and $x_5$ in this system, as from linear algebra we know that if we were unable to solve for $x_2$, $x_3$ and $x_5$ we would also not be able to solve for $a$.  Now  once we have $a$ we can continue solving the system for the other variables, and doing exactly this gives us that $(x_1,x_2,x_5,a) equiv (1, 16, 28, 23) space (mod 35)$. 

#numbered_problem(10)[
Try to implement the Index Calculus algorithm (Algorithm 3 from the notes) in Sagemath. Here are some hints:

    (i) *list(primes(B))* gives you a list of all primes up to $B$;
    
    (ii) You can initialize the matrix to then the relations having $b$ columns and $b+1$ rows;
    
    (iii) You do not need to factor completely, just divide out the primes in your factor base, then if you get 1 it means the number was completely factored;
    
    (iv) You can use *A.solve_right(b)* to solve the linear system $A x = b$ in $FF_q$.
]

Here is my implementation, code can be found in the scripts folder of my github repo under index_calculus.ipynb:

#figure(
  image("images/indexcalc1.png", width: 100%)
)
#figure(
  image("images/indexcalc2.png", width: 100%),
  caption: [ This implementation sees high variance in computation times (see graphs in problem $12$) most likely due to the fact that gathering relations is based on random chance, and sometimes the system is not solvable with our relations, so we must restart the algorithm and look for new relations.
],
)

#numbered_problem(11)[
Let $T_B$ be the expected number of trials of random integers modulo $p$ until one is $B$-smooth, $b$ be the number of primes up to $B$, $r$ is the (prime) order of $g$ and $M(t)$ be the number of bit operations required to multiply two $t$-bit integers. 

Then, the expected running time of the Index Calculus algorithm (using naive trial division and specialized methods for the *sparse* linear system) is:
#align(center, $O(b^2 T_B M(log(p)) + b^(2+o(1))M(log(r))) "for" p -> infinity$)


Let $L_p(1/2,c)$ be the subexponential function defined as
#align(center, $L_p(1/2,c) = e^(c sqrt(log(p) log(log(p))))$)


Assuming that $B = L_p(1/2,c)$ for some constant $c > 0$ and $T_B = L_p(1/2,1/(2c) + o(1))$, show that the optimal value for $c$ is $1/2$. Then estimate the asymptotic complexity of the Index Calculus algorithm.
]

Since we are only estimating asymptotic complexity we can ignore the solving of the sparse linear system, as this is very easy compared to gathering relations. Observe then that $b approx frac(B,log(B)) approx B= L_(p(c,1/2))$ (since $log(B) = c sqrt(log(p)log(log(p)))$ is negligible when we talk about asymptotics). Then #align(center, $b^2 T_B M(log(p)) = B^2 T_B M(log(p)) = L_(p(1/2,2c)) L_p(1/2,1/(2c) + o(1)) M(log(p)) =$) #align(center, $=e^((2c+1/(2c)+o(1))sqrt(log(p)log(log(p))))$) Now as $p --> oo$ we have that $o(1)$ is negligible and so we just need to choose $c$ such that $2c+1/(2c)$ is minimal. By calculus we know that $c=1/2$ is the positive $c$ which minimizes $2c+1/(2c)$, showing that the optimal $c$ is $1/2$. Then the asymptotic complexity of the Index Calculus algorithm is #align(center, $O(b^2M(T_B (log(p))+(log(r))))=O(M e^(2 sqrt(log(p)log(log(p))))log(p)+e^(sqrt(log(p)log(log(p))))log(r)))$) #align(center, $= O(L_(p(1/2,2)) M (log(p)))$)
Where we used the fact that the dominating term comes from gathering the relations, $b^2 T_B M(log(p))$.

#numbered_problem(12)[
Use your implementation of the Index Calculus algorithm to run some experiments and find an optimal value for $B$ for 
#align(center, $p = 2 dot 386545163 + 1, space space space g = 4$)

Does your experimental optimal value for $B$ agree with the asymptotic value you found in the previous exercise? How does it change? Try to use larger primes and find the optimal $B = L_p(1/2,c)$ for your implementation experimentally.

]

By our above calculations the optimal $B$ should be $⌈e^(1/2 sqrt(log(p)log(log(p))))⌉ = 51$. We will now run some experiments to determine that experimentally, where we will run our Index Calculus implementation for every $B in {20,21, dots, 175}$ and time our algorithm, this is by no means rigorous, but for the proof of the optimal value see the previous problem.

#figure(
  image("images/index_calc_time.png", width: 100%),
  caption: [
    We see that the optimal value for $B$ in this case is between $50-120$, with the exact being hard to pick out due to possible randomness in finding relations in order to solve the linear system.
  ],
)

For a test with a larger prime, we use the first prime in exersize $\# 3$, $p = 8589935363$. The expected optimal value of $B$ is $69$, now running our experiment again we get:

#figure(
  image("images/index_calc_time2.png", width: 100%),
  caption: [
    We see that the optimal value for $B$ in this case is between $60-120$. From the large spikes before $60$ it is clear that using the same $B$ for larger primes is not optimal.]
)